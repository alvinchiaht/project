# Predicting Loan Default using Lending Club Dataset

**Part 2: Modelling**

Load library and dataset

```{R}
pacman::p_load(dplyr, tidyverse, ggplot2, reshape2, car, caret, ggpubr, DescTools, ROCR,
               xgboost, rpart, rattle, nnet, randomForest, boot)
```

Set working directory and read csv files

```{R}
setwd("C:/Users/Alvin/Documents/WorkDirectory")

loans_df = read.csv("loansformodelling.csv",stringsAsFactors = TRUE)
tofactor = c("targetloanstatus","creditpolicy","term")
loans_df[,tofactor] = lapply(loans_df[,tofactor], as.factor)
```

**Sampling**

Create our training set using stratified sampling  
Partition dataset into training and test dataset in ratio 70:30

```{R}
set.seed(123)

inds = createDataPartition(1:nrow(loans_df), p=0.7, list=FALSE,times=1)

loans_dftrain = loans_df[inds,]
nrow(loans_dftrain)/nrow(loans_df)
dim(loans_dftrain)

loans_dftest = loans_df[-inds,]
nrow(loans_dftest)/nrow(loans_df)
dim(loans_dftest)

# use caret to upsample the train dataset
loans_dftrainUP = upSample(loans_dftrain, y = as.factor(loans_dftrain$targetloanstatus), list = TRUE)[[1]]
glimpse(loans_dftrainUP)

# use caret to downsample the train dataset
loans_dftrainDN = downSample(loans_dftrain, y = as.factor(loans_dftrain$targetloanstatus), list = TRUE)[[1]]
glimpse(loans_dftrainDN)
```

**Modelling**

(1) Logistic Regression

```{R}
loans_dfglm <- glm(formula = targetloanstatus ~ .,
                   family=binomial,  data=loans_dftrain)
summary(loans_dfglm)
vif(loans_dfglm)
```

Refine model using step function

```{R}
loans_dfglm2 = step(loans_dfglm, trace = F)
summary(loans_dfglm2)
vif(loans_dfglm2)
# vif < 3 for all variables
```

Check logliklihood

```{R}
attach(loans_dfglm2)
pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)
anova
formula # check current formula after step function.
detach(loans_dfglm2)

# Use anova to check remaining variables.
anova(loans_dfglm2, test="Chisq")

# Identifies emplength as insignificant variables. Retrain model by removing emplength 

loans_dfglm3 <- glm(formula = update.formula(loans_dfglm2, ~ . - emplength),
                    family=binomial,  data=loans_dftrain)
summary(loans_dfglm3)
vif(loans_dfglm3)
```

vif < 3 for all variables

Test model on trainset and check accuracy with confusion matrix

```{R}
pdataglm_train <- predict(loans_dfglm3, newdata = loans_dftrain, type = "response")

confusionMatrix(data = as.factor(as.numeric(pdataglm_train>0.5)), reference = loans_dftrain$targetloanstatus, positive="1")
```

Accuracy of trainset is 84.8%

Show Variable Importance

```{R}
VarImp_glm = as.data.frame(varImp(loans_dfglm3)) %>%
  rownames_to_column("Variable")
VarImp_glm %>% 
  ggplot(aes(x = reorder(Variable, Overall), y = Overall))+ geom_col() + 
  coord_flip() + labs(title = "Relative Importance of Variables for glm", x = 'Variable', y = 'Relative Importance')
```

Perform prediction on testset and look at confusion matrix.

```{R}
pdataglm_test <- predict(loans_dfglm3, newdata = loans_dftest, type = "response")

confusionMatrix(data = as.factor(as.numeric(pdataglm_test>0.5)), reference = loans_dftest$targetloanstatus,positive="1")
```

Accuracy of test set is 84.9% which is comparable to our training set

(2) Bagged logistic regression model  

Create function

```{R}
bagglm = function(model, agg = 10) {
  # takes a glm model input and returns a list of aggregate of models.
  df = model[["model"]]
  formula = formula(model)
  modellist = list()
  for ( i in 1:10) {
    a = sample_n(df, 0.9*nrow(df), replace = TRUE)
    modellist[[length(modellist) + 1]] = glm(formula, family=binomial,  data=a)
  }
  return(modellist)
}

predictbag = function(model, test_df, method = "mean") {
  # method = max or mean
  predictlist = data.frame()[1:nrow(test_df),]
  for (i in model) {
    predictlist = cbind(predictlist,predict(i, newdata = test_df, type = "response"))
  }
  if (method == "max") {
    predictlist$p = apply(predictlist,1,max)
  } else {
    predictlist$p = apply(predictlist,1,mean)
  }
  return(predictlist$p)
}
```

Apply bagging logistic regression 
Apply model on trainset and check accuracy with confusion matrix

```{R}
loans_dfglmbag = bagglm(loans_dfglm3, agg = 10)

pdataglmbag_train = predictbag(loans_dfglmbag,loans_dftrain, method = "mean")
confusionMatrix(data = as.factor(as.numeric(pdataglmbag_train>0.5)), reference = loans_dftrain$targetloanstatus,positive="1")
```

Accuracy on train set is 84.9%

Perform prediction on testset and look at confusion matrix

```{R}
pdataglmbag_test = predictbag(loans_dfglmbag,loans_dftest, method = "mean")
confusionMatrix(data = as.factor(as.numeric(pdataglmbag_test>0.5)), reference = loans_dftest$targetloanstatus,positive="1")
```

Accuracy on test set is 84.9%

Show variable Importance 

```{R}
VarImp_glmbag = lapply(loans_dfglmbag,varImp)
VarImp_glmbag = do.call(cbind,VarImp_glmbag)
VarImp_glmbag = apply(VarImp_glmbag,1,mean)
VarImp_glmbag = as.data.frame(VarImp_glmbag) %>%
  `colnames<-`("Overall") %>%
  rownames_to_column("Variable")

VarImp_glmbag %>% 
  ggplot(aes(x = reorder(Variable, Overall), y = Overall))+ geom_col() + 
  coord_flip() + labs(title = "Relative Importance of Variables for glmbag", x = 'Variable', y = 'Relative Importance')
```

(3) Decision Tree

Model using unbalanced dataset

```{R}
loans_dfrpart <- rpart(formula = targetloanstatus ~ .,
                       data=loans_dftrain,
                       method = "class",
                       parms=list(split="information"),
                       control= rpart.control(minsplit=20,
                                              minbucket=7,
                                              usesurrogate=0, 
                                              maxsurrogate=0),
                       model=TRUE)
```

Error encountered 
rpart not giving good information gain to split the trees on the unbalanced data set  
Try balancing the data before modelling  
Perform modelling on downsampled dataset

```{R}
loans_dfrpart <- rpart(formula = targetloanstatus ~ .,
                       data=loans_dftrainDN,
                       method = "class",
                       parms=list(split="information"),
                       control= rpart.control(minsplit=20,
                                              minbucket=7,
                                              usesurrogate=0, 
                                              maxsurrogate=0),
                       model=TRUE)
```

Generate a textual view of the Decision Tree model  
Show variable importance

```{R}
rattle::fancyRpartPlot(loans_dfrpart)
summary(loans_dfrpart)

loans_dfrpart[["variable.importance"]]
```

Test model on trainset and check accuracy with confusion matrix.

```{R}
pdata_traintree = predict(loans_dfrpart, loans_dftrainDN, type = "class")
confusionMatrix(pdata_traintree, reference = loans_dftrainDN$targetloanstatus,positive="1")
```

Accuracy of trainset is 62.4%

Perform prediction on testset and look at confusion matrix.

```{R}
pdata_tree = predict(loans_dfrpart, loans_dftest, type = "class")
confusionMatrix(pdata_tree, reference = loans_dftest$targetloanstatus)
```

Accuracy of trainset is 55.24% 

Show variable importance

```{R}
varimp_tree = as.data.frame(varImp(loans_dfrpart)) %>%
  `colnames<-`(c("importance")) %>%
  rownames_to_column("Variable")
varimp_tree %>%
  ggplot(aes(x = reorder(Variable, importance), y = importance))+ geom_col() + 
  coord_flip() + labs(title = "Relative Importance of Variables for Decision Tree", x = 'Variable', y = 'Relative Importance')
```

Calculate probabilities for ROC curve

```{R}
pdata_tree = predict(loans_dfrpart, loans_dftest, type = "prob")
```

(4) Random Forest

Build a forest with 1000 trees  
Use sqrt of total number of variables for mtry. Hence try mtry=4 

```{R}
st = Sys.time() 
set.seed(123)
rf_dn <- randomForest(targetloanstatus~., loans_dftrainDN,
                      ntree = 1000,
                      mtry = 4,
                      importance = TRUE,
                      cutoff=c(0.5,1-0.5),
                      na.action=na.exclude)
Sys.time()-st #35secs
plot(rf_dn)

#error stabilies at ntree = 400

st=Sys.time()
t <- tuneRF(loans_dftrainDN[,-6], loans_dftrainDN[,6],
            stepFactor = 0.5,
            plot = TRUE,
            ntreeTry = 400,
            trace = TRUE,
            improve = 0.05)
Sys.time()-st 

# optimum mtry=2

st = Sys.time() 
set.seed(123)
rf_dn <- randomForest(targetloanstatus~., loans_dftrainDN,
                      ntree = 400,
                      mtry = 2,
                      importance = TRUE,
                      cutoff=c(0.5,1-0.5),
                      na.action=na.exclude)
Sys.time()-st 
```

Test model on trainset and check accuracy with confusion matrix

```{R}
pdatarf_train_cm <- predict(rf_dn, newdata = loans_dftrainDN, type = "response")
confusionMatrix(data = pdatarf_train_cm, reference = loans_dftrainDN$targetloanstatus)
```

Accuracy of training set is 99.9%

Perform prediction on testset and look at confusion matrix.
```{R}
pdatarf_test_cm <- predict(rf_dn, newdata = loans_dftest, type = "response")
confusionMatrix(data = pdatarf_test_cm, reference = loans_dftest$targetloanstatus)
```

Accuracy of test set is 64.87%

Calculate probabilities for ROC curve
```{R}
pdatarf_test= predict(rf_dn, newdata = loans_dftest, type = "prob")
```

Show variable importance

```{R}
varimp_rf = as.data.frame(varImp(rf_dn)) %>%
  `colnames<-`(c("importance","importance2")) %>%
   select(-importance2) %>%
  rownames_to_column("Variable")
varimp_rf %>%
  ggplot(aes(x = reorder(Variable, importance), y = importance))+ geom_col() + 
  coord_flip() + labs(title = "Relative Importance of Variables for Random Forest", x = 'Variable', y = 'Relative Importance')
```

(5) Extreme Gradient Boosting Trees

```{R}
train_x = data.matrix(loans_dftrainDN[,-6])
train_y = loans_dftrainDN[,6]
train_y = ifelse(train_y=="1","1","0")
test_x = data.matrix(loans_dftest[,-6])
test_y = loans_dftest[,6]
test_y = ifelse(test_y=="1","1","0")

xgb_train = xgb.DMatrix(data=train_x, label=train_y)
xgb_test = xgb.DMatrix(data=test_x, label=test_y)

params_tree <- list(booster = "gbtree", 
                    eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1,
                    objective = "binary:logistic")
```

Xgboost cross validation

```{R}
set.seed(123)
xgbcv_tree = xgb.cv(data = xgb_train, 
                    params = params_tree, nrounds = 100, nfold = 5, 
                    showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)

# Identify iteration with lowest error for xgb tree

which.min((xgbcv_tree[["evaluation_log"]][["train_error_mean"]]))
which.min((xgbcv_tree[["evaluation_log"]][["test_error_mean"]]))
set.seed(123)
xgbc_tree <- xgb.train(data = xgb_train, 
                       params = params_tree, nfold = 5, nrounds = which.min((xgbcv_tree[["evaluation_log"]][["train_error_mean"]])), 
                       verbose = FALSE, eval_metric = 'auc')
```

Show relative importance

```{R}
mat_tree = xgb.importance(model=xgbc_tree)
xgb.plot.importance(importance_matrix = mat_tree[1:20],main="Relative Importance for XGboost Trees",cex=1,xlim=c(0,0.2)) 

```

Test on trainset and check confusion matrix

```{R}
x2_dn_traintree = predict(xgbc_tree, xgb_train, type="prob")
confusionMatrix(data = as.factor(as.numeric(x2_dn_traintree>0.5)), reference = loans_dftrainDN$targetloanstatus)
```

Accuracy = 90.4% for training set

Perform prediction on testset and look at confusion matrix

```{R}
x2_dn_tree = predict(xgbc_tree, xgb_test, type="prob")
confusionMatrix(data = as.factor(as.numeric(x2_dn_tree>0.5)), reference = loans_dftest$targetloanstatus)
```

Accuracy = 62.43% for test set

(6) Linear Extreme Gradient Boosting

```{R}
y = loans_dftrain$targetloanstatus
preProcess_range_model <- preProcess(loans_dftrain, method=c('center','scale'))
loans_dftrainBL =  predict(preProcess_range_model, newdata = loans_dftrain)
apply(loans_dftrainBL[, 1:17], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
loans_dftrainBLDN = downSample(loans_dftrainBL, y = as.factor(loans_dftrainBL$targetloanstatus), list = TRUE)[[1]]

preProcess_range_model_test <- preProcess(loans_dftest, method=c('center','scale'))
loans_dftestBL=  predict(preProcess_range_model_test, newdata = loans_dftest)
apply(loans_dftestBL[, 1:17], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})

trainBL_x = data.matrix(loans_dftrainBLDN[,-6])
trainBL_y = loans_dftrainBLDN[,6]
trainBL_y = ifelse(trainBL_y=="1","1","0")
testBL_x = data.matrix(loans_dftestBL[,-6])
testBL_y = loans_dftestBL[,6]
testBL_y = ifelse(testBL_y=="1","1","0")

xgb_train = xgb.DMatrix(data=trainBL_x, label=trainBL_y)
xgb_test = xgb.DMatrix(data=testBL_x, label=testBL_y)

params_linear = list(booster = "gblinear",
                     feature_selector = "shuffle", lambda = 1, alpha = 0,
                     objective = "binary:logistic")
```

Xgboost cross validation

```{R}
set.seed(123)
xgbcv_linear = xgb.cv(data = xgb_train, 
                      params = params_linear, nrounds = 100, nfold = 5, 
                      showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)

# identify iteration with lowest error for xgb linear

which.min((xgbcv_linear[["evaluation_log"]][["train_error_mean"]]))
which.min((xgbcv_linear[["evaluation_log"]][["test_error_mean"]]))

set.seed(123)
xgbc_linear <- xgb.train(data = xgb_train, 
                         params = params_linear, nfold = 5, nrounds = which.min((xgbcv_linear[["evaluation_log"]][["train_error_mean"]])), 
                         verbose = FALSE)
```

Test on trainset and check confusion matrix

```{R}
x2_dn_trainlinear = predict(xgbc_linear, xgb_train, type="prob")
confusionMatrix(data = as.factor(as.numeric(x2_dn_trainlinear>0.5)), reference = loans_dftrainDN$targetloanstatus,positive="1")
```

Accuracy = 62.91% for training set

Perform prediction on testset and look at confusion matrix

```{R}
x2_dn_linear = predict(xgbc_linear, xgb_test, type="prob")
confusionMatrix(data = as.factor(as.numeric(x2_dn_linear>0.5)), reference = loans_dftest$targetloanstatus,positive="1")
```

Accuracy = 62.98% for test set

Show relative importance

```{R}
mat_linear = xgb.importance(model=xgbc_linear)
xgb.plot.importance(importance_matrix = mat_linear[1:20],main="Coefficient XGboost Linear",cex=1) 
```

(7) Neural Network

Normalise numerical variables

```{R}
y = loans_dftrain$targetloanstatus
preProcess_range_model <- preProcess(loans_dftrain, method='range')
loans_dftrain =  predict(preProcess_range_model, newdata = loans_dftrain)
apply(loans_dftrain[, 1:17], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
```

One-Hot Encoding for categoral variables

```{R}
dummies_model = dummyVars(targetloanstatus ~ ., data=loans_dftrain)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat

loans_dftrain_mat <- predict(dummies_model, newdata = loans_dftrain)
loans_dftrainNN <- data.frame(loans_dftrain_mat)
loans_dftrainNN$targetloanstatus = y
str(loans_dftrainNN)
```

Use caret package to downsample the train dataset

```{R}
loans_dftrainNNDN = downSample(loans_dftrainNN, y = as.factor(loans_dftrainNN$targetloanstatus), list = TRUE)[[1]]
glimpse(loans_dftrainNNDN)
```


Define neural network parameter

Combine the attributes name for convenience

```{R}
names <- colnames(loans_dftrainNNDN)
f <- as.formula(paste("targetloanstatus ~", paste(names[!names %in% "targetloanstatus"], collapse = " + ")))
```

Train model

```{R}
set.seed(123)
st = Sys.time() 
nnmodel <- train(f, loans_dftrainNNDN, method='nnet', trace = FALSE,
                 #Grid of tuning parameters to try:
                 tuneGrid=expand.grid(.size=seq(1, 5, by = 1),.decay=c(0,0.001,0.1))) 
Sys.time() - st

#a 27-1-1 network with 30 weights 

# show neural network result

nnmodel[["finalModel"]]
plot(nnmodel)
```

Test model on training set 

```{R}
my_datatrain <- subset(loans_dftrainNNDN, select = -c(targetloanstatus)) 
predictNN_train_cm <- predict(nnmodel, my_datatrain, type = "raw")

confusionMatrix(data = predictNN_train_cm, reference = loans_dftrainNNDN$targetloanstatus)
```

Accuracy = 64.04%


Use confusion matrix to evaluate model performance on test data.

Data preparation  
Normalise numerical variables  
One-Hot Encoding for categorical variables

```{R}
z = loans_dftest$targetloanstatus
preProcess_range_model_test <- preProcess(loans_dftest, method='range')
loans_dftest=  predict(preProcess_range_model_test, newdata = loans_dftest)
apply(loans_dftest[, 1:17], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})

dummies_model = dummyVars(targetloanstatus ~ ., data=loans_dftest)

# Create the dummy variables using predict.
loans_dftest_mat <- predict(dummies_model, newdata = loans_dftest)
loans_dftestNN <- data.frame(loans_dftest_mat)
loans_dftestNN$targetloanstatus = z
```

Test model on test set

```{R}
predictNN_test_cm <- predict(nnmodel, loans_dftestNN, type = "raw")
predictNN_test <- predict(nnmodel, loans_dftestNN, type = "prob")

confusionMatrix(data = predictNN_test_cm, reference = loans_dftestNN$targetloanstatus,positive="1")
```

Accuracy = 57.41%

Show relative importance

```{R}
VarImp_nn = varImp(nnmodel)
VarImp_nn %>% 
  ggplot(aes(x = names, y = overall))+ geom_bar(stat ='identity') + coord_flip() + labs(title = "Relative Importance of Variables", x = "Variable", y = "Relative Importance")
```

Combine modelling results into a dataframe

```{R}
foreval = cbind(loans_dftest,
                pvalue_glm = pdataglm_test,
                pvalue_bag = pdataglmbag_test,
                pvalue_tree=pdata_tree[,2], 
                pvalue_forest=pdatarf_test[,2],
                pvalue_boosttree=x2_dn_tree,
                pvalue_boostlinear=x2_dn_linear,
                pvalue_NN=predictNN_test[,2])

write.csv(foreval, "foreval.csv", row.names = F)
```



